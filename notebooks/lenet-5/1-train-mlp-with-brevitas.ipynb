{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Quantized Lenet5 on MNIST with Brevitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: tqdm in /tmp/home_dir/.local/lib/python3.8/site-packages (4.66.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorboard in /tmp/home_dir/.local/lib/python3.8/site-packages (2.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (0.35.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (2.23.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /tmp/home_dir/.local/lib/python3.8/site-packages (from tensorboard) (1.60.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /tmp/home_dir/.local/lib/python3.8/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /tmp/home_dir/.local/lib/python3.8/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /tmp/home_dir/.local/lib/python3.8/site-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /tmp/home_dir/.local/lib/python3.8/site-packages (from tensorboard) (1.24.1)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /tmp/home_dir/.local/lib/python3.8/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /tmp/home_dir/.local/lib/python3.8/site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (50.3.1.post20201107)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (6.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /tmp/home_dir/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard) (3.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset <a id=\"train_qnn\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "train_val_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=False, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root=\"./datasets\", train=False, download=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Calculate mean and std\n",
    "imgs = torch.stack([img for img, _ in train_val_dataset], dim=0)\n",
    "\n",
    "mean = imgs.view(1, -1).mean(dim=1)    # or imgs.mean()\n",
    "std = imgs.view(1, -1).std(dim=1)     # or imgs.std()\n",
    "\n",
    "mnist_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "train_val_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=False, transform=mnist_transforms)\n",
    "test_dataset = datasets.MNIST(root=\"./datasets/\", train=False, download=False, transform=mnist_transforms)\n",
    "\n",
    "train_size = int(0.9 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset=train_val_dataset, lengths=[train_size, val_size])\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a PyTorch Device <a id='define_pytorch_device'></a> \n",
    "\n",
    "GPUs can significantly speed-up training of deep neural networks. We check for availability of a GPU and if so define it as target device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Quantized MLP Model <a id='define_quantized_mlp'></a>\n",
    "\n",
    "We'll now define an MLP model that will be trained to perform inference with quantized weights and activations.\n",
    "For this, we'll use the quantization-aware training (QAT) capabilities offered by [Brevitas](https://github.com/Xilinx/brevitas).\n",
    "\n",
    "Our MLP will have four fully-connected (FC) layers in total: three hidden layers with 64 neurons, and a final output layer with a single output, all using 2-bit weights. We'll use 2-bit quantized ReLU activation functions, and apply batch normalization between each FC layer and its activation.\n",
    "\n",
    "In case you'd like to experiment with different quantization settings or topology parameters, we'll define all these topology settings as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lenet5(\n",
       "  (feature): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): Tanh()\n",
       "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class lenet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            #1\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),   # 28*28->32*32-->28*28\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),  # 14*14\n",
    "            \n",
    "            #2\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),  # 10*10\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),  # 5*5\n",
    "            \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=16*5*5, out_features=120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.feature(x))\n",
    "\n",
    "model = lenet5()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "lenet5 (lenet5)                          [1, 1, 28, 28]       [1, 10]              --                   True\n",
       "├─Sequential (feature)                   [1, 1, 28, 28]       [1, 16, 5, 5]        --                   True\n",
       "│    └─Conv2d (0)                        [1, 1, 28, 28]       [1, 6, 28, 28]       156                  True\n",
       "│    └─Tanh (1)                          [1, 6, 28, 28]       [1, 6, 28, 28]       --                   --\n",
       "│    └─AvgPool2d (2)                     [1, 6, 28, 28]       [1, 6, 14, 14]       --                   --\n",
       "│    └─Conv2d (3)                        [1, 6, 14, 14]       [1, 16, 10, 10]      2,416                True\n",
       "│    └─Tanh (4)                          [1, 16, 10, 10]      [1, 16, 10, 10]      --                   --\n",
       "│    └─AvgPool2d (5)                     [1, 16, 10, 10]      [1, 16, 5, 5]        --                   --\n",
       "├─Sequential (classifier)                [1, 16, 5, 5]        [1, 10]              --                   True\n",
       "│    └─Flatten (0)                       [1, 16, 5, 5]        [1, 400]             --                   --\n",
       "│    └─Linear (1)                        [1, 400]             [1, 120]             48,120               True\n",
       "│    └─Tanh (2)                          [1, 120]             [1, 120]             --                   --\n",
       "│    └─Linear (3)                        [1, 120]             [1, 84]              10,164               True\n",
       "│    └─Tanh (4)                          [1, 84]              [1, 84]              --                   --\n",
       "│    └─Linear (5)                        [1, 84]              [1, 10]              850                  True\n",
       "========================================================================================================================\n",
       "Total params: 61,706\n",
       "Trainable params: 61,706\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.42\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.25\n",
       "Estimated Total Size (MB): 0.30\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=model, input_size=(1, 1, 28, 28), col_width=20,\n",
    "                  col_names=['input_size', 'output_size', 'num_params', 'trainable'], row_settings=['var_names'], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the QNN <a id=\"train_qnn\"></a>\n",
    "\n",
    "We provide two options for training below: you can opt for training the model from scratch (slower) or use a pre-trained model (faster). The first option will give more insight into how the training process works, while the second option will likely give better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece2f284d1d64652ac9e03aad185c4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Train loss:  0.21497| Train acc:  0.93767| Val loss:  0.07848| Val acc:  0.97507\n",
      "Epoch: 1| Train loss:  0.07235| Train acc:  0.97740| Val loss:  0.05275| Val acc:  0.98421\n",
      "Epoch: 2| Train loss:  0.05167| Train acc:  0.98376| Val loss:  0.04760| Val acc:  0.98388\n",
      "Epoch: 3| Train loss:  0.04124| Train acc:  0.98676| Val loss:  0.04755| Val acc:  0.98504\n",
      "Epoch: 4| Train loss:  0.03346| Train acc:  0.98919| Val loss:  0.04873| Val acc:  0.98554\n",
      "Epoch: 5| Train loss:  0.02875| Train acc:  0.99115| Val loss:  0.05294| Val acc:  0.98438\n",
      "Epoch: 6| Train loss:  0.02547| Train acc:  0.99167| Val loss:  0.04079| Val acc:  0.98853\n",
      "Epoch: 7| Train loss:  0.02272| Train acc:  0.99232| Val loss:  0.04954| Val acc:  0.98554\n",
      "Epoch: 8| Train loss:  0.01831| Train acc:  0.99361| Val loss:  0.04276| Val acc:  0.98853\n",
      "Epoch: 9| Train loss:  0.01691| Train acc:  0.99458| Val loss:  0.04437| Val acc:  0.98737\n",
      "Epoch: 10| Train loss:  0.01536| Train acc:  0.99469| Val loss:  0.04605| Val acc:  0.98770\n",
      "Epoch: 11| Train loss:  0.01332| Train acc:  0.99552| Val loss:  0.04265| Val acc:  0.98803\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Experiment tracking\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "experiment_name = \"MNIST\"\n",
    "model_name = \"LeNet5V1\"\n",
    "log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# device-agnostic setup\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#accuracy = accuracy.to(device)\n",
    "#model_lenet5v1 = model_lenet5v1.to(device)\n",
    "\n",
    "EPOCHS = 12\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # Training loop\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        y_pred = model(X) \n",
    "        y_predt = []\n",
    "        for weight in y_pred:\n",
    "            y_predt.append(torch.argmax(weight))\n",
    "\n",
    "        #print(y_predt,y)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        acc = accuracy_score(y_predt, y)\n",
    "        train_acc += acc\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "        \n",
    "    # Validation loop\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    model.eval()\n",
    "    #with torch.inference_mode():\n",
    "    for X, y in val_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "            \n",
    "        y_pred = model(X)\n",
    "\n",
    "        y_predt = []\n",
    "        for weight in y_pred:\n",
    "            y_predt.append(torch.argmax(weight))\n",
    "            \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        val_loss += loss.item()\n",
    "            \n",
    "        acc = accuracy_score(y_predt, y)\n",
    "        val_acc += acc\n",
    "            \n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_acc /= len(val_dataloader)\n",
    "        \n",
    "    writer.add_scalars(main_tag=\"Loss\", tag_scalar_dict={\"train/loss\": train_loss, \"val/loss\": val_loss}, global_step=epoch)\n",
    "    writer.add_scalars(main_tag=\"Accuracy\", tag_scalar_dict={\"train/acc\": train_acc, \"val/acc\": val_acc}, global_step=epoch)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the QNN <a id=\"train_qnn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.04169| Test acc:  0.98722\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = 0, 0\n",
    "\n",
    "model.eval()\n",
    "for X, y in test_dataloader:\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    y_pred = model(X)\n",
    "\n",
    "    y_predt = []\n",
    "    for weight in y_pred:\n",
    "        y_predt.append(torch.argmax(weight))\n",
    "        \n",
    "    test_loss += loss_fn(y_pred, y)\n",
    "    test_acc += accuracy_score(y_predt, y)\n",
    "        \n",
    "test_loss /= len(test_dataloader)\n",
    "test_acc /= len(test_dataloader)\n",
    "\n",
    "print(f\"Test loss: {test_loss: .5f}| Test acc: {test_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Brevitas model to disk\n",
    "torch.save(model.state_dict(), \"lenet5.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to FINN-ONNX <a id=\"export_finn_onnx\" ></a>\n",
    "\n",
    "\n",
    "[ONNX](https://onnx.ai/) is an open format built to represent machine learning models, and the FINN compiler expects an ONNX model as input. We'll now export our network into ONNX to be imported and used in FINN for the next notebooks. Note that the particular ONNX representation used for FINN differs from standard ONNX, you can read more about this [here](https://finn.readthedocs.io/en/latest/internals.html#intermediate-representation-finn-onnx).\n",
    "\n",
    "You can see below how we export a trained network in Brevitas into a FINN-compatible ONNX representation. Note how we create a `QuantTensor` instance with dummy data to tell Brevitas how our inputs look like, which will be used to set the input quantization annotation on the exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to lenet5.onnx\n"
     ]
    }
   ],
   "source": [
    "import brevitas.onnx as bo\n",
    "import numpy as np\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "\n",
    "ready_model_filename = \"lenet5.onnx\"\n",
    "input = torch.ones([1, 1, 28, 28], dtype = torch.float32)\n",
    "\n",
    "#Move to CPU before export\n",
    "model.cpu()\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    model, input, ready_model_filename\n",
    ")\n",
    "\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare FINN & Brevitas execution <a id=\"compare_brevitas\"></a>\n",
    "\n",
    "Load  brevitas model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brevitas_model = lenet5()\n",
    "trained_state_dict = torch.load(\"lenet5.pth\")\n",
    "brevitas_model.load_state_dict(trained_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_brevitas(current_inp):\n",
    "    output = torch.argmax(brevitas_model(current_inp))\n",
    "    brevitas_output = output.detach().numpy().tolist()\n",
    "    #print(brevitas_output[0])\n",
    "    return  brevitas_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model in .onnx format, we can work with it using FINN. To import it into FINN, we'll use the [`ModelWrapper`](https://finn.readthedocs.io/en/latest/source_code/finn.core.html#qonnx.core.modelwrapper.ModelWrapper). It is a wrapper around the ONNX model which provides several helper functions to make it easier to work with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "ready_model_filename = \"lenet5.onnx\"\n",
    "model_for_sim = ModelWrapper(ready_model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: input.1\n",
      "Output tensor name: 24\n",
      "Input tensor shape: [1, 1, 28, 28]\n",
      "Output tensor shape: [1, 10]\n",
      "Input tensor datatype: FLOAT32\n",
      "Output tensor datatype: FLOAT32\n",
      "List of node operator types in the graph: \n",
      "['Conv', 'Tanh', 'Pad', 'AveragePool', 'Conv', 'Tanh', 'Pad', 'AveragePool', 'Flatten', 'Gemm', 'Tanh', 'Gemm', 'Tanh', 'Gemm']\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = model_for_sim.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = model_for_sim.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = model_for_sim.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in model_for_sim.graph.node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model_for_sim = model_for_sim.transform(InferShapes())\n",
    "model_for_sim = model_for_sim.transform(FoldConstants())\n",
    "model_for_sim = model_for_sim.transform(GiveUniqueNodeNames())\n",
    "model_for_sim = model_for_sim.transform(GiveReadableTensorNames())\n",
    "model_for_sim = model_for_sim.transform(InferDataTypes())\n",
    "model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "verif_model_filename = \"lenet5.onnx\"\n",
    "model_for_sim.save(verif_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "def inference_with_finn_onnx(current_inp):\n",
    "    finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "    finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "    finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "    # convert input to numpy for FINN\n",
    "    current_inp = current_inp.detach().numpy()\n",
    "    # reshape to expected input (add 1 for batch dimension)\n",
    "    current_inp = current_inp.reshape(finnonnx_model_in_shape)\n",
    "    # create the input dictionary\n",
    "    input_dict = {finnonnx_in_tensor_name : current_inp} \n",
    "    # run with FINN's execute_onnx\n",
    "    output_dict = oxe.execute_onnx(model_for_sim, input_dict)\n",
    "    #get the output tensor\n",
    "    finn_outputs = output_dict[finnonnx_out_tensor_name] \n",
    "    #return finn_output\n",
    "    finn_outputs = finn_outputs.tolist()\n",
    "    finn_output = finn_outputs[0].index(max(finn_outputs[0]))\n",
    "    return finn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok =  9998\n",
      "nok =  0\n"
     ]
    }
   ],
   "source": [
    "# See random images with their labels\n",
    "torch.manual_seed(42)  # setting random seed\n",
    "\n",
    "ok = 0\n",
    "nok = 0\n",
    "\n",
    "for i in range(1, len(test_dataset) - 1):\n",
    "    img, label_gt = test_dataset[i]\n",
    "    img_temp = img.unsqueeze(dim=0).to(device)\n",
    "    brevitas_output = inference_with_brevitas(img_temp)\n",
    "    finn_output = inference_with_finn_onnx(img)\n",
    "    # compare the outputs\n",
    "    ok += 1 if finn_output == brevitas_output else 0\n",
    "    nok += 1 if finn_output != brevitas_output else 0\n",
    "\n",
    "print(\"ok = \", ok)\n",
    "print(\"nok = \", nok)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
